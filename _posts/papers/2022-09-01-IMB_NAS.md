---
layout: paper
categories: papers
permalink: papers/imb_nas
id: IMB-NAS
title: "IMB-NAS: Neural Architecture Search for Imbalanced Datasets"
authors:
  - Rahul Duggal
  - ShengYun Peng
  - Hao Zhou
  - Duen Horng Chau
venue: arXiv
year: 2022
url: /papers/imb_nas
pdf: /papers/22_imb_nas.pdf
type: paper
figure: /images/papers/22_imb_nas.png
feature-title: IMB-NAS&#58; Neural Architecture Search for Imbalanced Datasets
feature-description: 
image: /images/featured/22_eccv_skelevision.png
featured: false
feature-order: 
selected: false
type: misc
doi: 
bibtex: |-

  @article{duggal2022imb,
    title={IMB-NAS: Neural Architecture Search for Imbalanced Datasets},
    author={Duggal, Rahul and Peng, Shengyun and Zhou, Hao and Chau, Duen Horng},
    journal={arXiv preprint arXiv:2210.00136},
    year={2022}
  }
---

Class imbalance is a ubiquitous phenomenon occurring in real world data distributions. To overcome its detrimental effect on training accurate classifiers, existing work follows three major directions: class re-balancing, information transfer, and representation learning. In this paper, we propose a new and complementary direction for improving performance on long tailed datasets - optimizing the backbone architecture through neural architecture search (NAS). We find that an architecture's accuracy obtained on a balanced dataset is not indicative of good performance on imbalanced ones. This poses the need for a full NAS run on long tailed datasets which can quickly become prohibitively compute intensive. To alleviate this compute burden, we aim to efficiently adapt a NAS super-network from a balanced source dataset to an imbalanced target one. Among several adaptation strategies, we find that the most effective one is to retrain the linear classification head with reweighted loss, while freezing the backbone NAS super-network trained on a balanced source dataset. We perform extensive experiments on multiple datasets and provide concrete insights to optimize architectures for long tailed datasets.
